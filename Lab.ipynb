{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPezxdgnqNxhLA8aOTLnChu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BecomeAllan/RNN/blob/main/Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdgNtVwJtz4b"
      },
      "source": [
        "# Laboratório\n",
        "\n",
        "## Implementações com RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdZzGJUQlJ3i"
      },
      "source": [
        "### Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqnJPUzMrbNx"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "\n",
        "!pip install unidecode\n",
        "from unidecode import unidecode\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-bCz5eclRam"
      },
      "source": [
        "## Tratamento de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAfxkvsat4A0",
        "outputId": "8414a533-9ef0-4356-c386-c859e4d360c1"
      },
      "source": [
        "# Baixar dataset\n",
        "!gdown --id 15FhPHu7Hx6ul_k-EEBZwzpUWznK0gBR3\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15FhPHu7Hx6ul_k-EEBZwzpUWznK0gBR3\n",
            "To: /content/portuguese-poems.csv\n",
            "13.3MB [00:00, 77.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "rlafdjkFuxw4",
        "outputId": "f6fdf06f-fcb0-4088-e9e3-09a1a1a0588e"
      },
      "source": [
        "# Data1\n",
        "df1 = pd.read_csv(\"portuguese-poems.csv\", encoding='UTF-8')\n",
        "\n",
        "df1 = df1.dropna()\n",
        "\n",
        "\n",
        "df1.head()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Author</th>\n",
              "      <th>Title</th>\n",
              "      <th>Content</th>\n",
              "      <th>Views</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cecília Meireles</td>\n",
              "      <td>Retrato</td>\n",
              "      <td>Eu não tinha este rosto de hoje,\\r\\nAssim calm...</td>\n",
              "      <td>1018431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Fernando Pessoa</td>\n",
              "      <td>Para ser grande, sê inteiro: nada</td>\n",
              "      <td>Para ser grande, sê inteiro: nada\\r\\nTeu exage...</td>\n",
              "      <td>1979413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Marina Colasanti</td>\n",
              "      <td>Eu sei, mas não devia</td>\n",
              "      <td>Eu sei que a gente se acostuma. Mas não devia....</td>\n",
              "      <td>301509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Carlos Drummond de Andrade</td>\n",
              "      <td>Quadrilha</td>\n",
              "      <td>João amava Teresa que amava Raimundo\\r\\nque am...</td>\n",
              "      <td>1421206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Eugénio de Andrade</td>\n",
              "      <td>É urgente o amor</td>\n",
              "      <td>É urgente o amor.\\r\\nÉ urgente um barco no mar...</td>\n",
              "      <td>621197</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Author  ...    Views\n",
              "0            Cecília Meireles  ...  1018431\n",
              "1             Fernando Pessoa  ...  1979413\n",
              "2            Marina Colasanti  ...   301509\n",
              "3  Carlos Drummond de Andrade  ...  1421206\n",
              "4          Eugénio de Andrade  ...   621197\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pirlCPfptiF5",
        "outputId": "3881f6ad-c8c6-4c98-b7c2-3e1b47792cd8"
      },
      "source": [
        "df1['Title'] = df1['Title'].apply(unidecode).str.lower()\n",
        "df1['Content'] = df1['Content'].apply(unidecode).str.lower()\n",
        "\n",
        "\n",
        "data = pd.DataFrame()\n",
        "\n",
        "data['Econder_inputs'] = df1['Title']\n",
        "#.apply(lambda row: re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", row)).apply(vectorize_string)\n",
        "data['Decoder_inputs'] = df1['Content'].apply(lambda row: row[:-1])\n",
        "#.apply(lambda row: re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", row))\n",
        "data['Decoder_targets'] = df1['Content'].apply(lambda row: row[1:])\n",
        "#.apply(lambda row: re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", row)).apply(lambda row: vectorize_string(row[1:]))\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "\n",
        "tokenizer.fit_on_sequences(data['Decoder_inputs'])\n",
        "\n",
        "dictionary = tokenizer.word_index\n",
        "\n",
        "\n",
        "\n",
        "# Transforma uma string em um array de idxs\n",
        "def vectorize_string(string):\n",
        "  vectorized_output = [dictionary[char] for char in string]\n",
        "  return vectorized_output\n",
        "\n",
        "\n",
        "tokenizer.texts_to_sequences(data['Econder_inputs'][0])\n",
        "\n",
        "vectorized_songs = vectorize_string(Content_joined[0:30])\n",
        "print(vectorized_songs)\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "dictionary[2]\n",
        "\n",
        "tokenizer.fit_on_sequences(train_data['Content'])\n",
        "\n",
        "tokenizer.texts_to_sequences(train_data['Content'][1])\n",
        "\n",
        "pad_sequences()"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FviOrtB45y4",
        "outputId": "3d05fdf0-4094-4095-c416-c65282ddcb86"
      },
      "source": [
        "print(idx2char[vectorized_songs])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['e' 'u' ' ' 'n' 'a' 'o' ' ' 't' 'i' 'n' 'h' 'a' ' ' 'e' 's' 't' 'e' ' '\n",
            " 'r' 'o' 's' 't' 'o' ' ' 'd' 'e' ' ' 'h' 'o' 'j']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OvEtyZy8jXy"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "\n",
        "## Encoder\n",
        "encode_input = keras.Input(shape=(None,), name=\"title\")\n",
        "encode_features = layers.Embedding(vocab_size, embedding_dim)(encode_input) \n",
        "encoder = layers.LSTM(100, return_state=True, name = 'encode')\n",
        "encode_output, state_h, state_c = encoder(encode_features)\n",
        "\n",
        "# Estado da celula \n",
        "encoder_state = [state_h, state_c]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Decoder\n",
        "decode_input = keras.Input(shape=(None,), name=\"content\")\n",
        "decode_features = layers.Embedding(vocab_size, embedding_dim)(decode_input)\n",
        "decode = layers.LSTM(100, return_state=True, return_sequences=True, name = 'decode')\n",
        "decode_out, _, _ = decode(decode_features, initial_state = encoder_state)\n",
        "decoder_outputs = layers.TimeDistributed(layers.Dense(vocab_size, activation=\"softmax\"))(decode_out)\n",
        "\n",
        "# Estado da celula \n",
        "\n",
        "\n",
        "model = keras.Model([encode_input, decode_input], decoder_outputs)\n",
        "model.summary()\n",
        "\n",
        "#keras.backend.clear_session()\n",
        "\n",
        "keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-3),\n",
        "    loss = \"categorical_crossentropy\"\n",
        ")\n",
        "\n",
        "model.fit([train_data['Title'][1], train_data['Content']][1], train_data['Content_out'][1],\n",
        "          epochs=100)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}