{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMLe7HCjHuhd1gtT1ehZSIm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEWwC2NDUgPJ"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BecomeAllan/RNN/blob/main/Lab.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdgNtVwJtz4b"
      },
      "source": [
        "# Laboratório\n",
        "\n",
        "## Implementações com RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdZzGJUQlJ3i"
      },
      "source": [
        "### Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqnJPUzMrbNx"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "\n",
        "!pip install unidecode\n",
        "from unidecode import unidecode\n",
        "\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-bCz5eclRam"
      },
      "source": [
        "## Tratamento de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAfxkvsat4A0"
      },
      "source": [
        "# Baixar dataset\n",
        "!gdown --id 15FhPHu7Hx6ul_k-EEBZwzpUWznK0gBR3\n",
        "\n",
        "!gdown --id 1Eq9oi3_1PuSZ5hoZS5M1A5pTMSyy3K_a\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlafdjkFuxw4"
      },
      "source": [
        "# Data1\n",
        "df1 = pd.read_csv(\"portuguese-poems.csv\", encoding='UTF-8')\n",
        "\n",
        "# Data2\n",
        "df2 = pd.read_csv(\"Ethereum Historical Data.csv\", encoding='UTF-8')\n",
        "\n",
        "df1 = df1.dropna()\n",
        "df2 = df2.dropna()\n",
        "\n",
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pirlCPfptiF5"
      },
      "source": [
        "data = pd.DataFrame()\n",
        "\n",
        "\n",
        "Generative_inputs = '\\n\\n'.join(df1['Title'])\n",
        "\n",
        "\n",
        "df1['Title'] = df1['Title'].apply(unidecode).str.lower()\n",
        "df1['Content'] = df1['Content'].apply(unidecode).str.lower()\n",
        "\n",
        "# Criando os dados\n",
        "\n",
        "data['Econder_inputs'] = df1['Title']\n",
        "\n",
        "data['Decoder_inputs'] = df1['Content'].apply(lambda row: \"<BOS> \" + row[:-1])\n",
        "\n",
        "data['Decoder_targets'] = df1['Content'].apply(lambda row: row[1:] + \" <EOS>\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11wMYuGEYK-v"
      },
      "source": [
        "# Treina os Tokens\n",
        "tokenizer = Tokenizer(char_level=True, lower=False)\n",
        "\n",
        "tokenizer.fit_on_texts(data['Econder_inputs'])\n",
        "tokenizer.fit_on_texts(data['Decoder_targets'])\n",
        "tokenizer.fit_on_texts(data['Decoder_inputs'])\n",
        "\n",
        "dictionary = tokenizer.word_index\n",
        "\n",
        "k = tokenizer.texts_to_sequences(data['Econder_inputs'][0])\n",
        "k_text = tokenizer.sequences_to_texts(k)\n",
        "\n",
        "print(dictionary)\n",
        "print(f'O input: {data[\"Econder_inputs\"][0]}')\n",
        "print(f'O Token do input: {k}')\n",
        "print(f'O decode do Token do input: {k_text}')\n",
        "\n",
        "\n",
        "data['Econder_inputs'] = tokenizer.texts_to_sequences(data['Econder_inputs'])\n",
        "\n",
        "data['Decoder_inputs'] = tokenizer.texts_to_sequences(data['Decoder_inputs'])\n",
        "\n",
        "data['Decoder_targets'] = tokenizer.texts_to_sequences(data['Decoder_targets'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akrp1IGzah0n"
      },
      "source": [
        "# Seq-Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bos8pRQgaU09"
      },
      "source": [
        "# Treina os Tokens\n",
        "tokenizer1 = Tokenizer(char_level=True, lower=False)\n",
        "\n",
        "vocab = sorted(set(Generative_inputs))\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "\n",
        "idx2char = np.array(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3eQMB7YG4CC"
      },
      "source": [
        "def vectorize_string(string):\n",
        "  vectorized_output = np.array([char2idx[char] for char in string])\n",
        "  return vectorized_output\n",
        "\n",
        "def get_batch(vectorized_songs, seq_length, batch_size):\n",
        "  # the length of the vectorized songs string\n",
        "  n = vectorized_songs.shape[0] - 1\n",
        "  # randomly choose the starting indices for the examples in the training batch\n",
        "  idx = np.random.choice(n-seq_length, batch_size)\n",
        "\n",
        "  input_batch = [vectorized_songs[i : i+seq_length] for i in idx]\n",
        "  output_batch = [vectorized_songs[i+1 : i+seq_length+1] for i in idx]\n",
        "\n",
        "  # x_batch, y_batch provide the true inputs and targets for network training\n",
        "  x_batch = np.reshape(input_batch, [batch_size, seq_length])\n",
        "  y_batch = np.reshape(output_batch, [batch_size, seq_length])\n",
        "  return x_batch, y_batch\n",
        "\n",
        "\n",
        "\n",
        "X, Y = get_batch(vectorize_string(Generative_inputs), 30, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rxY8wc2eTJR"
      },
      "source": [
        "Model = keras.Sequential()\n",
        "\n",
        "Model.add(layers.Embedding(len(vocab), 250, batch_input_shape=[1000, None]))\n",
        "Model.add(layers.LSTM(100))\n",
        "Model.add(layers.Dense(len(vocab)))\n",
        "\n",
        "Model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-1),\n",
        "    loss = \"sparse_categorical_crossentropy\",\n",
        "    metrics = keras.metrics.categorical_accuracy\n",
        ")\n",
        "\n",
        "Y.shape\n",
        "\n",
        "Model.fit(x= X,y=Y, epochs=100)\n",
        "\n",
        "Model(X)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43FCVoZxTx0x"
      },
      "source": [
        "# Seq2value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrFvsM7XT47k"
      },
      "source": [
        "df2.head()\n",
        "\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "Model = keras.Sequential()\n",
        "\n",
        "Model.add(layers.LSTM(64, input_shape=(None, 3), return_sequences=True))\n",
        "Model.add(layers.LSTM(64))\n",
        "Model.add(layers.Dense(10, activation=keras.activations.elu))\n",
        "Model.add(BatchNormalization())\n",
        "Model.add(layers.Dense(1, activation='linear'))\n",
        "\n",
        "Model.summary()\n",
        "\n",
        "df2.columns\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(df2[[\"Open\",\"Low\", 'High']].to_numpy(), df2[['Price']].to_numpy(), test_size = 0.2)\n",
        "\n",
        "X_train[1]\n",
        "\n",
        "Model(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b06KTcSQGYDt"
      },
      "source": [
        "# Encode-Decode (Seq2Seq)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FviOrtB45y4"
      },
      "source": [
        "MAX_LENGTH_TITLE = 100\n",
        "MAX_LENGTH_POEM = 100\n",
        "\n",
        "data_Econder_inputs = pad_sequences(data['Econder_inputs'], maxlen=MAX_LENGTH_TITLE, padding='post', truncating='post')\n",
        "\n",
        "data_Decoder_inputs = pad_sequences(data['Decoder_inputs'], maxlen=MAX_LENGTH_POEM, padding='post', truncating='post')\n",
        "\n",
        "data_Decoder_targets = pad_sequences(data['Decoder_targets'], maxlen=MAX_LENGTH_POEM, padding='post', truncating='post')\n",
        "\n",
        "\n",
        "Encoder_Train, Encoder_Test, _, _ = train_test_split(data_Econder_inputs, data_Decoder_inputs, test_size = 0.2, random_state=5)\n",
        "Decoder_Train, Decoder_Test, Target_Train, Target_Test  =  train_test_split(data_Decoder_inputs, data_Decoder_targets, test_size = 0.2, random_state=5)\n",
        "\n",
        "Target_Train = keras.utils.to_categorical(Target_Train, num_classes=len(dictionary))\n",
        "Target_Test = keras.utils.to_categorical(Target_Test, num_classes=len(dictionary))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OvEtyZy8jXy"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(dictionary)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 76\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 50\n",
        "\n",
        "\n",
        "## Encoder\n",
        "encode_input = keras.Input(shape=(None,), name=\"title\")\n",
        "encode_features = layers.Embedding(vocab_size, embedding_dim)(encode_input) \n",
        "encoder = layers.LSTM(rnn_units, return_state=True, name = 'encode')\n",
        "encode_output, state_h, state_c = encoder(encode_features)\n",
        "\n",
        "# Estado da celula \n",
        "encoder_state = [state_h, state_c]\n",
        "\n",
        "\n",
        "\n",
        "#layers.CuDNNLSTM\n",
        "\n",
        "## Decoder\n",
        "decode_input = keras.Input(shape=(None,), name=\"content\")\n",
        "decode_features = layers.Embedding(vocab_size, embedding_dim)(decode_input)\n",
        "decode = layers.LSTM(rnn_units, return_state=True, return_sequences=True, name = 'decode')\n",
        "decode_out, _, _ = decode(decode_features, initial_state = encoder_state)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(decode_out)\n",
        "\n",
        "# Estado da celula \n",
        "\n",
        "\n",
        "model = keras.Model([encode_input, decode_input], decoder_outputs)\n",
        "model.summary()\n",
        "\n",
        "#keras.backend.clear_session()\n",
        "\n",
        "#keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-1),\n",
        "    loss = \"categorical_crossentropy\",\n",
        "    metrics = keras.metrics.categorical_accuracy\n",
        ")\n",
        "\n",
        "#class CustomCallback(keras.callbacks.Callback):\n",
        "#  def on_epoch_end(self, epoch, logs=None):\n",
        "#    if 0 == epoch%1: \n",
        "#        print(f'Época: {epoch} \\n output:{logs[\"predict\"]}')\n",
        "        \n",
        "\n",
        "model.fit([Encoder_Train, Decoder_Train], Target_Train,\n",
        "          batch_size = 3000,\n",
        "          epochs=2, verbose=2)\n",
        "\n",
        "pred = model.predict([Encoder_Test[0:2], Decoder_Test[0:2]])\n",
        "\n",
        "# Predições \n",
        "preds = np.argmax(pred[0], axis=1)\n",
        "\n",
        "np.delete(pred, preds)\n",
        "\n",
        "\n",
        "enc = tokenizer.sequences_to_texts([ [idx] for idx in Encoder_Test[0].tolist()])\n",
        "res = tokenizer.sequences_to_texts([ [idx] for idx in pred.tolist()])\n",
        "\n",
        "print(f'Encode: {\"\".join(enc)}')\n",
        "print(\"\".join(res))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}