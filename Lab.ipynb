{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOvpaZAi5Tezou+c/GcWN30",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BecomeAllan/RNN/blob/main/Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdgNtVwJtz4b"
      },
      "source": [
        "# Laboratório\n",
        "\n",
        "## Implementações com RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdZzGJUQlJ3i"
      },
      "source": [
        "### Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqnJPUzMrbNx"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "\n",
        "!pip install unidecode\n",
        "from unidecode import unidecode\n",
        "\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-bCz5eclRam"
      },
      "source": [
        "## Tratamento de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAfxkvsat4A0"
      },
      "source": [
        "# Baixar dataset\n",
        "!gdown --id 15FhPHu7Hx6ul_k-EEBZwzpUWznK0gBR3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlafdjkFuxw4"
      },
      "source": [
        "# Data1\n",
        "df1 = pd.read_csv(\"portuguese-poems.csv\", encoding='UTF-8')\n",
        "\n",
        "df1 = df1.dropna()\n",
        "\n",
        "\n",
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pirlCPfptiF5"
      },
      "source": [
        "df1['Title'] = df1['Title'].apply(unidecode).str.lower()\n",
        "df1['Content'] = df1['Content'].apply(unidecode).str.lower()\n",
        "\n",
        "# Creado os dados\n",
        "data = pd.DataFrame()\n",
        "\n",
        "data['Econder_inputs'] = df1['Title']\n",
        "#.apply(lambda row: re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", row)).apply(vectorize_string)\n",
        "data['Decoder_inputs'] = df1['Content'].apply(lambda row: \"<BOS> \" + row[:-1])\n",
        "#.apply(lambda row: re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", row))\n",
        "data['Decoder_targets'] = df1['Content'].apply(lambda row: row[1:] + \" <EOS>\")\n",
        "#.apply(lambda row: re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", row)).apply(lambda row: vectorize_string(row[1:]))"
      ],
      "execution_count": 454,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11wMYuGEYK-v",
        "outputId": "457d6eb1-5d53-47d2-b1fd-a1656ed924e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Treina os Tokens\n",
        "tokenizer = Tokenizer(char_level=True, lower=False)\n",
        "\n",
        "tokenizer.fit_on_texts(data['Econder_inputs'])\n",
        "tokenizer.fit_on_texts(data['Decoder_targets'])\n",
        "tokenizer.fit_on_texts(data['Decoder_inputs'])\n",
        "\n",
        "dictionary = tokenizer.word_index\n",
        "\n",
        "k = tokenizer.texts_to_sequences(data['Econder_inputs'][0])\n",
        "k_text = tokenizer.sequences_to_texts(k)\n",
        "\n",
        "print(dictionary)\n",
        "print(f'O input: {data[\"Econder_inputs\"][0]}')\n",
        "print(f'O Token do input: {k}')\n",
        "print(f'O decode do Token do input: {k_text}')\n",
        "\n",
        "\n",
        "data['Econder_inputs'] = tokenizer.texts_to_sequences(data['Econder_inputs'])\n",
        "\n",
        "data['Decoder_inputs'] = tokenizer.texts_to_sequences(data['Decoder_inputs'])\n",
        "\n",
        "data['Decoder_targets'] = tokenizer.texts_to_sequences(data['Decoder_targets'])"
      ],
      "execution_count": 456,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 456
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FviOrtB45y4"
      },
      "source": [
        "MAX_LENGTH_TITLE = 100\n",
        "MAX_LENGTH_POEM = 250\n",
        "\n",
        "data_Econder_inputs = pad_sequences(data['Econder_inputs'], maxlen=MAX_LENGTH_TITLE, padding='post', truncating='post')\n",
        "\n",
        "data_Decoder_inputs = pad_sequences(data['Decoder_inputs'], maxlen=MAX_LENGTH_POEM, padding='post', truncating='post')\n",
        "\n",
        "data_Decoder_targets = pad_sequences(data['Decoder_targets'], maxlen=MAX_LENGTH_POEM, padding='post', truncating='post')\n",
        "\n",
        "\n",
        "Encoder_Train, Encoder_Test, _, _ = train_test_split(data_Econder_inputs, data_Decoder_inputs, test_size = 0.2, random_state=5)\n",
        "Decoder_Train, Decoder_Test, Target_Train, Target_Test  =  train_test_split(data_Decoder_inputs, data_Decoder_targets, test_size = 0.2, random_state=5)\n",
        "\n",
        "Target_Train = keras.utils.to_categorical(Target_Train, num_classes=len(dictionary))\n",
        "Target_Test = keras.utils.to_categorical(Target_Test, num_classes=len(dictionary))\n"
      ],
      "execution_count": 419,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OvEtyZy8jXy"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(dictionary)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 76\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 50\n",
        "\n",
        "\n",
        "## Encoder\n",
        "encode_input = keras.Input(shape=(None,), name=\"title\")\n",
        "encode_features = layers.Embedding(vocab_size, embedding_dim)(encode_input) \n",
        "encoder = layers.LSTM(rnn_units, return_state=True, name = 'encode')\n",
        "encode_output, state_h, state_c = encoder(encode_features)\n",
        "\n",
        "# Estado da celula \n",
        "encoder_state = [state_h, state_c]\n",
        "\n",
        "\n",
        "\n",
        "#layers.CuDNNLSTM\n",
        "\n",
        "## Decoder\n",
        "decode_input = keras.Input(shape=(None,), name=\"content\")\n",
        "decode_features = layers.Embedding(vocab_size, embedding_dim)(decode_input)\n",
        "decode = layers.LSTM(rnn_units, return_state=True, return_sequences=True, name = 'decode')\n",
        "decode_out, _, _ = decode(decode_features, initial_state = encoder_state)\n",
        "decoder_outputs = layers.TimeDistributed(layers.Dense(vocab_size, activation=\"softmax\"))(decode_out)\n",
        "\n",
        "# Estado da celula \n",
        "\n",
        "\n",
        "model = keras.Model([encode_input, decode_input], decoder_outputs)\n",
        "model.summary()\n",
        "\n",
        "#keras.backend.clear_session()\n",
        "\n",
        "#keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-3),\n",
        "    loss = \"categorical_crossentropy\",\n",
        "    metrics = keras.metrics.categorical_accuracy\n",
        ")\n",
        "\n",
        "#class CustomCallback(keras.callbacks.Callback):\n",
        "#  def on_epoch_end(self, epoch, logs=None):\n",
        "#    if 0 == epoch%1: \n",
        "#        print(f'Época: {epoch} \\n output:{logs[\"predict\"]}')\n",
        "        \n",
        "\n",
        "model.fit([Encoder_Train, Decoder_Train], Target_Train,\n",
        "          batch_size = 3000,\n",
        "          epochs=2, verbose=2)\n",
        "\n",
        "pred = model.predict([Encoder_Test[0:2], Decoder_Test[0:2]])\n",
        "\n",
        "# Predições \n",
        "\n",
        "pred = np.argmax(pred[0], axis=1)\n",
        "\n",
        " \n",
        "enc = tokenizer.sequences_to_texts([ [idx] for idx in Encoder_Test[0].tolist()])\n",
        "res = tokenizer.sequences_to_texts([ [idx] for idx in pred.tolist()])\n",
        "\n",
        "print(f'Encode: {\"\".join(enc)}')\n",
        "print(\"\".join(res))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}